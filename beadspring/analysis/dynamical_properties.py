import numpy as np

def compute_msd(positions, per_particle=False):
    '''
    Computes the mean squared displacement and the mean squared displacement per particle
    for a given trajectory. Note that you have to provide unwrapped coordinates.

    Parameters
    ----------
    positions : np.ndarray
        trajectory array with the shape (traj_length, N, 3)
    per_particle : bool
        If True, returns the mean squared displacement per particle.
        Default is False

    Returns
    -------
    msd : np.ndarray
        Mean squared displacement -> len (traj_length)
    msd_pp : np.ndarray
        MSD per particle -> shape(traj_length, N)
    '''

    dx2 = np.sum((positions[1:] - positions[0])**2, axis=2) # MSD per particle
    msd = np.mean(dx2, axis=1)    

    if per_particle:
        return msd, dx2
    else:
        return msd


def compute_ngp(positions):
    '''
    Computes the non-Gaussian parameter for a given trajectory. 
    Note that you have to provide unwrapped coordinates.

    Parameters
    ----------
    positions : np.ndarray
        trajectory array with the shape (traj_length, N, 3)

    Returns
    -------
    ngp : np.ndarray
        Non-Gaussian parameter -> len (traj_length)
    '''
    dx2 = np.sum((positions[1:] - positions[0])**2, axis=2)
    dx4 = np.sum((positions[1:] - positions[0])**4, axis=2)

    dr2 = np.mean(dx2, axis=1) # note that this is the MSD
    dr4 = np.mean(dx4, axis=1)

    ngp = 3.0 * dr4 / (5.0 * dr2**2) - 1.0
    return ngp


def compute_debye_waller_factor(time_log, msd, tau_p=3.0):
    '''
    Computes the Debye-Wallar factor from the mean squared displacement.
    The choice of time scale is set to be at 3 tau by default.

    Parameters
    ----------
    time_log : np.ndarray
        Logarithmically saved simulation times.
    msd : np.ndarray
        Mean squared displacement computed from 
        logarithmically saved trajectories.

    Returns
    -------
    dwf : float
        Mean squared displacement evaluated at 3 tau
        
    '''
    tau_p = np.where(time_log == tau_p)[0][0]
    dwf = msd[tau_p]
    return dwf


def compute_van_hove_correlation(positions, time_log, bins=100, rmax=8.0):
    #TODO: This is generated by copilot, please check if it is correct
    '''
    Computes the Van Hove correlation function for a given trajectory.
    Note that you have to provide unwrapped coordinates.

    Parameters
    ----------
    positions : np.ndarray
        trajectory array with the shape (traj_length, N, 3)
    time_log : np.ndarray
        Logarithmically saved simulation times.
    bins : int, optional
        Number of bins for the histogram. The default is 100.
    rmax : float, optional
        Maximum distance for the histogram. The default is 8.0.

    Returns
    -------
    r : np.ndarray
        Grid points of the Van Hove correlation function.
    g_r : np.ndarray
        The Van Hove correlation function.
    '''

    r_max = rmax
    r_min = 0.0
    dr = (r_max - r_min) / bins
    r = np.linspace(r_min, r_max, bins)

    # Compute the distance matrix
    distance_matrix = np.linalg.norm(positions[1:] - positions[0], axis=2)

    # Compute the histogram
    g_r, _ = np.histogram(distance_matrix, bins=bins, range=(r_min, r_max))

    # Normalize the histogram
    volume = 4.0 / 3.0 * np.pi * (r_max**3 - r_min**3)
    n_particles = positions.shape[1]
    g_r = g_r / (n_particles * volume)

    return r, g_r


def get_k_vectors(ktarget, box_length, save_vectors=False, memory_limit_gb=8.0):
    
    #!TODO: Update the below values accordingly to support non-cubic boxes
    k_step_x = 2 * np.pi / box_length
    k_step_y = 2 * np.pi / box_length
    k_step_z = 2 * np.pi / box_length
    k_discrete = ktarget / min(k_step_x, k_step_y, k_step_z)
    k_max = int(np.ceil(k_discrete))
    
    # Estimate memory usage
    num_elements = (k_max ** 3)  # Total elements in one array
    bytes_per_element = 4  # Assuming 64-bit integers
    total_bytes = num_elements * bytes_per_element * 3  # For kx, ky, kz
    total_gigabytes = total_bytes / (1024**3)

    if total_gigabytes >= memory_limit_gb:
        print(f"Estimated memory required: {total_gigabytes:.2f} GB exceeds {memory_limit_gb}GB, aborting...")
        return

    kx, ky, kz = np.meshgrid(np.arange(k_max), np.arange(k_max), np.arange(k_max), indexing='ij')
    magnitudes_squared = kx**2 + ky**2 + kz**2
    close_to_k_discrete = np.abs(np.sqrt(magnitudes_squared) - k_discrete) < 0.2

    valid_kx = kx[close_to_k_discrete]
    valid_ky = ky[close_to_k_discrete]
    valid_kz = kz[close_to_k_discrete]

    k_vectors = np.vstack((k_step_x * valid_kx, k_step_y * valid_ky, k_step_z * valid_kz)).T
    print(f"Found {k_vectors.shape[0]} valid k-vectors.")

    if k_vectors.size == 0:
        print("No kvectors found!")
        return

    if save_vectors:
        np.save("k_vectors.npy", k_vectors)
    
    return k_vectors


def compute_fskt(positions, k_vectors):
    dr = positions[1:] - positions[0]
    dr_k = np.dot(dr, k_vectors.T)
    fskt = np.mean(np.cos(dr_k), axis=(1,2))
    return fskt