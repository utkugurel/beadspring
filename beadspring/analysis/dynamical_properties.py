import numpy as np
from itertools import product


def compute_msd(positions, per_particle=False):
    '''
    Computes the mean squared displacement and the mean squared displacement per particle
    for a given trajectory. Note that you have to provide unwrapped coordinates.

    Parameters
    ----------
    positions : np.ndarray
        trajectory array with the shape (traj_length, N, 3)
    per_particle : bool
        If True, returns the mean squared displacement per particle.
        Default is False

    Returns
    -------
    msd : np.ndarray
        Mean squared displacement -> len (traj_length)
    msd_pp : np.ndarray
        MSD per particle -> shape(traj_length, N)
    '''

    dx2 = np.sum((positions[1:] - positions[0])**2, axis=2) # MSD per particle
    msd = np.mean(dx2, axis=1)    

    if per_particle:
        return msd, dx2
    else:
        return msd


def compute_ngp(positions):
    '''
    Computes the non-Gaussian parameter for a given trajectory. 
    Note that you have to provide unwrapped coordinates.

    Parameters
    ----------
    positions : np.ndarray
        trajectory array with the shape (traj_length, N, 3)

    Returns
    -------
    ngp : np.ndarray
        Non-Gaussian parameter -> len (traj_length)
    '''
    dx2 = np.sum((positions[1:] - positions[0])**2, axis=2)
    dx4 = np.sum((positions[1:] - positions[0])**4, axis=2)

    dr2 = np.mean(dx2, axis=1) # note that this is the MSD
    dr4 = np.mean(dx4, axis=1)

    ngp = 3.0 * dr4 / (5.0 * dr2**2) - 1.0
    return ngp


def compute_debye_waller_factor(time_log, msd, tau_p=3.0):
    '''
    Computes the Debye-Wallar factor from the mean squared displacement.
    The choice of time scale is set to be at 3 tau by default.

    Parameters
    ----------
    time_log : np.ndarray
        Logarithmically saved simulation times.
    msd : np.ndarray
        Mean squared displacement computed from 
        logarithmically saved trajectories.

    Returns
    -------
    dwf : float
        Mean squared displacement evaluated at 3 tau
        
    '''
    tau_p = np.where(time_log == tau_p)[0][0]
    dwf = msd[tau_p]
    return dwf


def compute_van_hove_correlation(positions, time_log, bins=100, rmax=8.0):
    #TODO: This is generated by copilot, please check if it is correct
    '''
    Computes the Van Hove correlation function for a given trajectory.
    Note that you have to provide unwrapped coordinates.

    Parameters
    ----------
    positions : np.ndarray
        trajectory array with the shape (traj_length, N, 3)
    time_log : np.ndarray
        Logarithmically saved simulation times.
    bins : int, optional
        Number of bins for the histogram. The default is 100.
    rmax : float, optional
        Maximum distance for the histogram. The default is 8.0.

    Returns
    -------
    r : np.ndarray
        Grid points of the Van Hove correlation function.
    g_r : np.ndarray
        The Van Hove correlation function.
    '''

    r_max = rmax
    r_min = 0.0
    dr = (r_max - r_min) / bins
    r = np.linspace(r_min, r_max, bins)

    # Compute the distance matrix
    distance_matrix = np.linalg.norm(positions[1:] - positions[0], axis=2)

    # Compute the histogram
    g_r, _ = np.histogram(distance_matrix, bins=bins, range=(r_min, r_max))

    # Normalize the histogram
    volume = 4.0 / 3.0 * np.pi * (r_max**3 - r_min**3)
    n_particles = positions.shape[1]
    g_r = g_r / (n_particles * volume)

    return r, g_r


def get_k_vectors(ktarget, box_length, max_points=1000, save_vectors=False, memory_limit_gb=8.0):
    
    #!TODO: Update the below values accordingly to support non-cubic boxes
    k_step_x = 2 * np.pi / box_length
    k_step_y = 2 * np.pi / box_length
    k_step_z = 2 * np.pi / box_length
    k_discrete = ktarget / min(k_step_x, k_step_y, k_step_z)
    k_max = int(np.ceil(k_discrete))
    
    # Estimate memory usage
    num_elements = (k_max ** 3)  # Total elements in one array
    bytes_per_element = 4  # Assuming 64-bit integers
    total_bytes = num_elements * bytes_per_element * 3  # For kx, ky, kz
    total_gigabytes = total_bytes / (1024**3)

    if total_gigabytes >= memory_limit_gb:
        print(f"Estimated memory required: {total_gigabytes:.2f} GB exceeds {memory_limit_gb}GB, aborting...")
        return

    kx, ky, kz = np.meshgrid(np.arange(k_max), np.arange(k_max), np.arange(k_max), indexing='ij')
    magnitudes_squared = kx**2 + ky**2 + kz**2
    close_to_k_discrete = np.abs(np.sqrt(magnitudes_squared) - k_discrete) < 0.2

    valid_kx = kx[close_to_k_discrete]
    valid_ky = ky[close_to_k_discrete]
    valid_kz = kz[close_to_k_discrete]


    # Generate all sign permutations
    signs = np.array(list(product([-1, 1], repeat=3)))
    all_vectors = []

    for sign in signs:
        signed_vectors = np.column_stack([
            sign[0] * valid_kx * k_step_x,
            sign[1] * valid_ky * k_step_y,
            sign[2] * valid_kz * k_step_z
        ])
        all_vectors.append(signed_vectors)

    k_vectors = np.vstack(all_vectors)

    print(f"Found {k_vectors.shape[0]} valid k-vectors.")

    if k_vectors.size == 0:
        print("No kvectors found!")
        return
    
    # Sample k-vectors if necessary
    if len(k_vectors) > max_points:
        np.random.seed(42)
        k_vectors = k_vectors[np.random.choice(len(k_vectors), max_points, replace=False)]

    if save_vectors:
        np.save("k_vectors.npy", k_vectors)
    
    return k_vectors


def compute_fskt(positions, k_vectors):
    dr = positions[1:] - positions[0]
    dr_k = np.dot(dr, k_vectors.T)
    fskt = np.mean(np.cos(dr_k), axis=(1,2))
    return fskt